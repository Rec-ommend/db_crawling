{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capstone_db_download.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFtjLB4hwyQE"
      },
      "source": [
        "# db 정보 csv에 저장/불러오기 함수선언\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# csv read\n",
        "def make_dataframe(csv_path):\n",
        "  if os.path.exists(csv_path):\n",
        "    dataframe = pd.read_csv(csv_path)\n",
        "    print(\"DataFrame loaded.\")\n",
        "    return dataframe\n",
        "  else:\n",
        "    dataframe = pd.DataFrame({\n",
        "        'id': [],             # youtube id\n",
        "        'title': [],          # 노래제목\n",
        "        'singer': [],         # 가수이름(,으로 여러명 구분)\n",
        "    })\n",
        "    print(\"DataFrame created.\")\n",
        "  return dataframe\n",
        "\n",
        "# csv read 예시\n",
        "# csv_path = \"/content/drive/My Drive/crawling.csv\"\n",
        "# dataframe = make_dataframe(csv_path)\n",
        "\n",
        "# csv write 예시\n",
        "# dataframe.to_csv(csv_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV4YPdTLcJYd"
      },
      "source": [
        "# 멜론 연간 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
        "    }\n",
        "params = {\n",
        "    'chartType': \"YE\",\n",
        "    'year': \"2019\",\n",
        "    'classCd': \"KPOP\", \n",
        "    'moved':'Y'}\n",
        "\n",
        "titles = []\n",
        "singers = []\n",
        "albumids = []\n",
        "songids = []\n",
        "\n",
        "for year in range(2000, 2020):\n",
        "  url = \"https://www.melon.com/chart/search/list.htm\"\n",
        "  params['year'] = year;\n",
        "  res = requests.get(url, params=params, headers=headers)\n",
        "  html = res.text\n",
        "  \n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "  title_list = [title.text.strip() for title in soup.select('.ellipsis.rank01 strong')]\n",
        "  singer_list = [singer.text.strip() for singer in soup.select('.checkEllipsis')]\n",
        "  album_list = [''.join(filter(str.isdigit, albumid['href'])) for albumid in soup.select('.ellipsis.rank03 a')]\n",
        "  songid_list = [''.join(filter(str.isdigit, albumid['href'])) for albumid in soup.select('.t_left .wrap a.btn.btn_icon_detail')]\n",
        "  print(res.url)\n",
        "  print(len(title_list), title_list)\n",
        "  print(len(singer_list), singer_list)\n",
        "  print(len(album_list), album_list)\n",
        "  print(len(songid_list), songid_list)\n",
        "\n",
        "  titles += title_list\n",
        "  singers += singer_list\n",
        "  albumids += album_list\n",
        "  songids += songid_list\n",
        "\n",
        "  res.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6NYNX97pamB"
      },
      "source": [
        "# 멜론 월간 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.melon.com/chart/search/list.htm\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
        "    }\n",
        "params = {\n",
        "          'chartType': \"MO\",\n",
        "          'classCd': \"DM0000\",\n",
        "          'mon': \"01\",\n",
        "          'year': \"2020\",\n",
        "          'moved':'Y'\n",
        "          }\n",
        "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
        "\n",
        "titles = []\n",
        "singers = []\n",
        "albumids = []\n",
        "songids = []\n",
        "\n",
        "for mon in months:\n",
        "  params['mon'] = mon;\n",
        "  res = requests.get(url, params=params, headers=headers)\n",
        "  html = res.text\n",
        "  \n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "  title_list = [title.text.strip() for title in soup.select('.ellipsis.rank01 strong')]\n",
        "  singer_list = [singer.text.strip() for singer in soup.select('.checkEllipsis')]\n",
        "  album_list = [''.join(filter(str.isdigit, albumid['href'])) for albumid in soup.select('.ellipsis.rank03 a')]\n",
        "  songid_list = [''.join(filter(str.isdigit, albumid['href'])) for albumid in soup.select('.t_left .wrap a.btn.btn_icon_detail')]\n",
        "  print(res.url)\n",
        "  print(len(title_list), title_list)\n",
        "  print(len(singer_list), singer_list)\n",
        "  print(len(album_list), album_list)\n",
        "  print(len(songid_list), songid_list)\n",
        "  \n",
        "  titles += title_list\n",
        "  singers += singer_list\n",
        "  albumids += album_list\n",
        "  songids += songid_list\n",
        "\n",
        "  res.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPqGKtRqfTe2"
      },
      "source": [
        "# 엘범 메타 정보 크롤링\n",
        "url_base = 'https://www.melon.com/song/detail.htm?songId='\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'ko-KR',\n",
        "    'Pragma': 'no-cache',\n",
        "    }\n",
        "csv_path = \"/content/drive/My Drive/crawling.csv\"\n",
        "dataframe = make_dataframe(csv_path)\n",
        "\n",
        "\n",
        "res.close()\n",
        "for i in range(len(albumids)):\n",
        "  title = titles[i]\n",
        "  singer = singers[i]\n",
        "\n",
        "  temp = dataframe[(dataframe['title'] == title) & (dataframe['singer'] == singer)]\n",
        "  target_index = temp.index[0]\n",
        "\n",
        "  if pd.notna(dataframe['release'][target_index]):\n",
        "    print(f'pass {target_index}')\n",
        "    continue\n",
        "  res = requests.get(url_base + songids[i], headers=headers)\n",
        "  soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "  meta = soup.select('.meta .list dd')\n",
        "  print(res.url, res.status_code, title)\n",
        "  dataframe['release'][target_index] = meta[1].text\n",
        "  dataframe['genre'][target_index] = meta[2].text\n",
        "\n",
        "  res.close()\n",
        "\n",
        "  dataframe.to_csv(csv_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bm29a8nRuGX"
      },
      "source": [
        "# dataframe에 맬론 크롤링 기록\n",
        "csv_path = \"/content/drive/My Drive/crawling.csv\"\n",
        "\n",
        "dataframe = make_dataframe(csv_path)\n",
        "\n",
        "for title, singer in zip(titles, singers): # title과 singer 배열이 제대로 차있는지 확인하고 실행바람\n",
        "  dataframe = dataframe.append({\n",
        "      'title': title,\n",
        "      'singer': singer\n",
        "  }, ignore_index=True)\n",
        "\n",
        "# 중복 삭제 및 csv 저장\n",
        "dataframe = dataframe.drop_duplicates(['title', 'singer'])\n",
        "dataframe.to_csv(csv_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-utqaWHYFG4X"
      },
      "source": [
        "# 유튜브 음원 크롤링\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRjfQgwf66cq",
        "outputId": "33bdc797-12a2-405f-da31-dfe0c2817d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 유튜브 크롤링 범위 정의 및 오디오 url 크롤링\n",
        "import numpy as np\n",
        "from urllib import parse\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"disable-gpu\")\n",
        "\n",
        "url_base = 'https://www.youtube.com/results?search_query='\n",
        "\n",
        "csv_path = \"/content/drive/My Drive/crawling.csv\"\n",
        "dataframe = make_dataframe(csv_path)\n",
        "pd.set_option('mode.chained_assignment',  None) # SettingWithCopyWarning 무시\n",
        "id_process = dataframe[\"id\"].notna()\n",
        "\n",
        "for i in range(dataframe.shape[0]):\n",
        "  if id_process[i]:\n",
        "    continue\n",
        "\n",
        "  query = parse.quote(f'{dataframe[\"title\"][i]} {dataframe[\"singer\"][i]}')\n",
        "  search_url = url_base + query\n",
        "\n",
        "  driver = webdriver.Chrome(options=chrome_options)\n",
        "  driver.get(search_url)\n",
        "  html = driver.page_source\n",
        "\n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "  video_id = soup.select('.yt-simple-endpoint.style-scope.ytd-video-renderer')[0]['href']\n",
        "  video_id = video_id[9:]\n",
        "  print(search_url)\n",
        "  print(video_id)\n",
        "  dataframe['id'][i] = video_id\n",
        "  driver.quit()\n",
        "  dataframe.to_csv(csv_path, index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w93gYuYU4n2A"
      },
      "source": [
        "# 음색 추출, 유튭 다운로드, 음성추출 라이브러리 설치\n",
        "!pip install timbral_models pydub pafy youtube_dl spleeter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9x_pKT_Ajpp"
      },
      "source": [
        "# 음원 다운로드 function\n",
        "import pafy\n",
        "import os\n",
        "\n",
        "def get_youtube_audio(video_id:str, save_path:str):\n",
        "  video = pafy.new(video_id)\n",
        "  audiostreams = video.audiostreams\n",
        "  # for s in audiostreams:\n",
        "  #   print(s)\n",
        "  # download_target = video.getbestaudio(preftype=\"webm\")\n",
        "  download_target = audiostreams[0]  #get worst audio\n",
        "  print('title:', download_target.title, ', bitrate:' , download_target.bitrate)\n",
        "  \n",
        "  filepath = os.path.join(save_path, video_id) + f\".{download_target.extension}\"\n",
        "  download_target.download(filepath)\n",
        "  return filepath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ8Kqlqn3faj"
      },
      "source": [
        "# 보컬 추출 (한 개)\n",
        "from spleeter.separator import Separator\n",
        "from spleeter.audio.adapter import AudioAdapter\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "separator = Separator('spleeter:2stems')\n",
        "\n",
        "def vocal_extract(youtube_id, input_path, output_path, codec=\"mp3\", bitrate='40k'):\n",
        "  if os.path.exists(input_path):\n",
        "    print(f\"Processing {input_path} ...\")\n",
        "    separator.separate_to_file(input_path, output_path, codec=codec, bitrate=bitrate)\n",
        "    return os.path.join(output_path, youtube_id, f'vocals.{codec}')\n",
        "  else:\n",
        "    print(f'File \"{input_path}\" is not exists.')\n",
        "\n",
        "\n",
        "# test\n",
        "# print(vocal_extract('abuTb3qMOsY', '/content/abuTb3qMOsY.webm', './'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLrGbp4Gkre3"
      },
      "source": [
        "# 유튜브 다운로드 & 음성추출 & 음색 추출 & 다운로드 제거\n",
        "import timbral_models\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "audio_folder = './'\n",
        "csv_path = \"/content/drive/My Drive/crawling.csv\"\n",
        "dataframe = make_dataframe(csv_path)\n",
        "\n",
        "for i in range(926, dataframe.shape[0]):\n",
        "  if pd.notna(dataframe['hardness'][i]):\n",
        "    print(f'pass id: {i}')\n",
        "    continue\n",
        "  id = dataframe['id'][i]\n",
        "\n",
        "  audio_name = audio_folder + id\n",
        "  try:\n",
        "    audio_file_path = get_youtube_audio(id, audio_folder) # 음원 다운로드\n",
        "  except:\n",
        "    print(f'get_youtube_audio failed : {id}')\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    vocal_path = vocal_extract(id, audio_file_path, './') # 음성 추출\n",
        "  except Exception as ex:\n",
        "    print(id, ex)\n",
        "    os.remove(audio_file_path) #음원 삭제\n",
        "    continue\n",
        "\n",
        "  new_vocal_format = 'wav'\n",
        "  new_vocal_path = audio_name+'_vocal.'+new_vocal_format\n",
        "  new_vocal_sample_rate = 22050\n",
        "\n",
        "  y, sr = librosa.load(vocal_path, new_vocal_sample_rate)\n",
        "  librosa.output.write_wav(new_vocal_path, y, sr) # format 변경\n",
        "\n",
        "  timbre = timbral_models.timbral_extractor(new_vocal_path)\n",
        "  print(i, timbre)\n",
        "  for attr, val in timbre.items():\n",
        "    dataframe[attr][i] = val\n",
        "\n",
        "  os.remove(audio_file_path) #음원 삭제\n",
        "  shutil.rmtree(audio_folder + id) #추출된 음성 삭제\n",
        "  os.remove(new_vocal_path) #음성 wav파일 삭제\n",
        "\n",
        "  dataframe.to_csv(csv_path, index=False) # dataframe 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAdLzif_QllJ"
      },
      "source": [
        "#db normalization\n",
        "import numpy as np\n",
        "\n",
        "csv_path = \"/content/drive/My Drive/crawling.csv\"\n",
        "meta_csv_path = \"/content/drive/My Drive/crawling_meta.csv\"\n",
        "dataframe = make_dataframe(csv_path)\n",
        "metadf = pd.read_csv(meta_csv_path)\n",
        "\n",
        "attrs = ['hardness', 'depth', 'brightness', 'roughness', 'warmth', 'sharpness', 'boominess']\n",
        "\n",
        "for i in range(dataframe.shape[0]):\n",
        "  for a in attrs:\n",
        "    metadf[a+\"_min\"][0] = min(metadf[a+\"_min\"][0], dataframe.loc[i][a])\n",
        "    metadf[a+\"_max\"][0] = max(metadf[a+\"_max\"][0], dataframe.loc[i][a])\n",
        "metadf.to_csv(meta_csv_path, index=False)\n",
        "\n",
        "dataframe[[attr + \"_norm\" for attr in attrs]] = None\n",
        "\n",
        "for i in range(dataframe.shape[0]):\n",
        "  for a in attrs:\n",
        "    minValue = metadf[a+\"_min\"][0]\n",
        "    maxValue = metadf[a+\"_max\"][0]\n",
        "    dataframe[a+\"_norm\"][i] = (dataframe[a][i] - minValue) / (maxValue - minValue)\n",
        "dataframe.to_csv(csv_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLWku6co-WHs"
      },
      "source": [
        "!pip install timbral_models pydub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQiqZIBSCE9W"
      },
      "source": [
        "import timbral_models\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "attrs = ['hardness', 'depth', 'brightness', 'roughness', 'warmth', 'sharpness', 'boominess']\n",
        "\n",
        "# 음색 유사도 반환\n",
        "def timbre_similarity(input_timbre, db_timbre):\n",
        "  similarity = 0\n",
        "  for attr in attrs:\n",
        "    similarity += (input_timbre[attr] - db_timbre[attr + \"_norm\"]) ** 2\n",
        "  return similarity\n",
        "\n",
        "# db에서 음색이 유사한 곡순으로 리스트 반환 \n",
        "def timbre_similarity_list(input_vocal_path:str, database_path:str, meta_db_path:str):\n",
        "\n",
        "  if not os.path.exists(database_path):\n",
        "    print(f'Database is not exists at {database_path}.')\n",
        "    return\n",
        "\n",
        "  dataframe = pd.read_csv(database_path)\n",
        "  dataframe['timbre_similarity'] = 0\n",
        "  \n",
        "\n",
        "  if not os.path.exists(meta_db_path):\n",
        "    print(f'Database is not exists at {meta_db_path}.')\n",
        "    return\n",
        "\n",
        "  meta_df = pd.read_csv(meta_db_path)\n",
        "\n",
        "\n",
        "  if not os.path.exists(input_vocal_path):\n",
        "    print(f'Audio file is not exists at {input_vocal_path}.')\n",
        "    return\n",
        "\n",
        "  timbre = timbral_models.timbral_extractor(input_vocal_path)\n",
        "  for a in attrs:\n",
        "    min = metadf[a+\"_min\"][0]\n",
        "    max = metadf[a+\"_max\"][0]\n",
        "    timbre[a] = (timbre[a] - min) / (max - min)\n",
        "  print(timbre)\n",
        "\n",
        "  norm_attrs = [attr + \"_norm\" for attr in attrs]\n",
        "  for index in range(dataframe.shape[0]):\n",
        "    dataframe['timbre_similarity'][index] = timbre_similarity(timbre, dict(dataframe[norm_attrs].loc[index]))\n",
        "  \n",
        "  new_df = dataframe.sort_values(by=['timbre_similarity'])\n",
        "  return new_df[['id', 'title', 'singer','genre', 'release', 'timbre_similarity']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8gXdEBS4kMF"
      },
      "source": [
        "# module test\n",
        "import timbral_models\n",
        "vocal_path = \"/content/drive/My Drive/vocal.wav\"\n",
        "db_path = \"/content/drive/My Drive/crawling.csv\"\n",
        "meta_db_path = \"/content/drive/My Drive/crawling_meta.csv\"\n",
        "\n",
        "sim_list = timbre_similarity_list(vocal_path, db_path, meta_db_path)\n",
        "sim_list.to_csv(\"/content/drive/My Drive/sim_list.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4YIHbNSMYX9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}